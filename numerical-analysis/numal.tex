\documentclass[12pt]{article}
\usepackage[margin=1in, headheight=20pt]{geometry}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{eso-pic}
\usepackage{amsthm, amsmath, amssymb}
\usepackage{mathtools}
\usepackage[italicdiff]{physics}
\usepackage{enumitem}
\usepackage{lmodern}
\usepackage{fancyhdr}
\usepackage{pgfornament}
\usepackage{parskip}
\usepackage{clrscode3e}

\definecolor{pagecolor}{HTML}{DCE2F0}
\definecolor{textcolor}{HTML}{373D4A}

\pagecolor{pagecolor}
\color{textcolor}

\AddToShipoutPictureBG{
  \begin{tikzpicture}[remember picture, overlay]
    \draw[
      line width=0.5pt,
      color=textcolor,
      opacity=0.075
    ]
    (current page.south west) grid[step=10pt] (current page.north east);
  \end{tikzpicture}
}


\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{Numerical Analysis}
\fancyhead[R]{\nouppercase{\leftmark}}
\fancyfoot[C]{}

\renewcommand{\headrule}{
  \vspace{-5pt}
  \hbox to \headwidth{
    \leaders\hrule height 0.5pt\hfill
    \hspace{5pt}
    \raisebox{0.20pt}{\pgfornament[width=1cm]{11}}
    \hspace{5pt}
    \leaders\hrule height 0.5pt\hfill
  }
}

\renewcommand{\footrule}{
  \vspace{-12pt}
  \hbox to \headwidth{
    \leaders\hrule height 3.5pt depth -3pt \hfill 
    \hspace{5pt} 
    \thepage 
    \hspace{5pt}
    \leaders\hrule height 3.5pt depth -3pt \hfill
  }
}

\fancypagestyle{plain}{
  \fancyhf{}
  \renewcommand{\headrulewidth}{0pt}
  \renewcommand{\headrule}{} 
  \fancyfoot[C]{}
  \renewcommand{\footrule}{
    \vspace{-12pt}
    \hbox to \headwidth{
      \rule[0.65ex]{0.47\headwidth}{0.5pt}%
      \hfill
      \thepage
      \hfill
      \rule[0.65ex]{0.47\headwidth}{0.5pt}%
    }
  }
}

\newcommand{\bb}[1]{\mathbb{#1}}
\newcommand{\cl}[1]{\mathcal{#1}}

\newcommand{\p}[1]{\left ( #1 \right )}
\newcommand{\bk}[1]{\left [ #1 \right ]}
\newcommand{\br}[1]{\left \{ #1 \}}
\newcommand{\ab}[1]{\langle #1 \rangle}

\newcommand{\f}[2]{\frac{#1}{#2}}
\newcommand{\nset}{\varnothing}
\newcommand{\oo}{\infty}
\DeclareMathOperator{\fl}{fl}
\newcommand{\NaN}{\texttt{NaN}}
\newcommand{\apx}{\approx}

\newcommand{\al}{\alpha}
\newcommand{\be}{\beta}
\newcommand{\gm}{\gamma}
\newcommand{\de}{\delta}
\newcommand{\De}{\Delta}
\newcommand{\ep}{\varepsilon}
\newcommand{\la}{\lambda}
\newcommand{\si}{\sigma}
\newcommand{\om}{\omega}
\newcommand{\Om}{\Omega}

\newcommand{\imp}{\Rightarrow}
\newcommand{\pmi}{\Leftarrow}
\renewcommand{\iff}{\Leftrightarrow}
\newcommand{\ffi}{\Rightarrow\!\Leftarrow}

\setlist[enumerate]{label=(\alph*)}

\newtheoremstyle{boldnote}
  {}
  {}
  {\itshape}
  {}
  {\bfseries}
  {.}
  { }
  {\thmname{#1}\thmnumber{ #2}\thmnote{ (\bfseries #3)}}
\theoremstyle{boldnote}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}

\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\newenvironment{solution}
  {\begin{proof}[Solution]}
  {\renewcommand{\qedsymbol}{$\blacksquare$}\end{proof}}


\title{
    \textbf{Numerical Analysis} \\
}
\author{
    Dhyan Laad \\
    \texttt{2024ADPS0875G}
}
\date{}

\begin{document}

\maketitle
\tableofcontents
\newpage

\section{Error Analysis}
\subsection{Floating Point Forms}
A real number may potentially have an infinite decimal expansion, but computers are limited by hardware, and as such store numbers with a terminating approximation.

\begin{definition}
    Given a real number $x$ with digits $d_1, d_2, \dots$, the \emph{$n$-digit, base $\beta$ floating point form}, or \emph{$n$-$\beta$ floating point form} is
    \[(-1)^s \times (0.d_1d_2\dots d_n)_\be \times \be^e\]
    where $s \in \{0, 1\}$ is the \emph{sign}, $e$ is the \emph{exponent}, and the $\be$-fraction
    \[(0.d_1d_2\dots d_n)_\be = \f{d_1}{\be^1} + \f{d_2}{\be^2} + \cdots + \f{d_n}{\be^n}\]
    is called the \emph{mantissa}. In the case that $d_1 \neq 0$, the representation is called the \emph{normalized floating point form}.
\end{definition}

For a fixed value of $\beta$ and $n$ as defined above, the notation $\fl (x)$ is used to denote the $n$-$\be$ floating point representation of $x$. Furthermore, for all computing systems, there are bounds on the values that the exponent $e$ can take. This leads to the concepts of underflow and overflow.

\begin{definition}
    Let a real number $x$ have a floating point form with exponent $e$. For a computing system with exponential range $(m , M)$ where $m$ and $M$ are integers,
    \begin{enumerate}
        \item if $e > M$, then the system is said to \emph{overflow}, and the result of the computation is denoted with a signed infinity: $\pm \oo$, and
        \item if $e < m$, then the system is said to \emph{underflow}, and the result of the computation is simply $0$.
    \end{enumerate}
\end{definition}

There are two ways to determine the mantissa of the floating point representation of a real number with more than $n$ digits: chopping and rounding. The chopped mantissa of the floating point representation of $x = 0.d_1d_2\dots d_nd_{n+1}\dots$ would simply be $(0.d_1d_2\dots d_n)$, while the rounded mantissa would be
\[\begin{cases}
    (0.d_1d_2\dots d_n) & d_{n+1} \in [0, \be/2), \\
    (0.d_1d_2\dots (d_n+1)) & d_{n+1} \in [\be/2, \be].
\end{cases}\]

\subsection{Errors}
The error of a floating point representation is a quantitification of how far removed it is from its true value.

\begin{definition}
    Let $x \in \bb R$. The \emph{absolute error} of its floating point representation is
    \[x - \fl(x).\]
\end{definition}

Note that since $\fl(x) \leq x$ for all $x \in \bb R$, the absolute error is always a positive quantity. Absolute error is the simplest quantitification but not the most useful, motivating a definition for relative error.

\begin{definition}
    The ratio of the absolute error to the true value of a real number $x$ is called its \emph{relative error}. It is customarily denoted with $\ep$:
    \[\ep = \f{x - \fl(x)}{x}.\]
\end{definition}

Another quantitification of how removed an approximation is from its true value is captured in the approximation's significant figures or significant digits.

\begin{definition}
    Let $x$ be a real number and $x^*$ be an approximation of it. Then if
    \[\abs{x - x^*} \leq \f 12 \be^{s-r+1}\]
    where $s$ is the largest integer such that $\be^s \leq \abs{x}$, then $x^*$ is said to approximate $x$ to $r$ \emph{significant figures} in $\be$.
\end{definition}

\begin{theorem}
    Let $\fl(x)$ be the $n$-$\be$ floating point representation for $x \in \bb R$, and set
    \[\ep = \f{x - \fl(x)}{x}.\]
    Then,
    \begin{enumerate}[font=\upshape]
        \item $\ep \leq \be^{-n+1}$ for chopped systems, and
        \item $\displaystyle \ep \leq \f 12 \be^{-n+1}$ for rounded systems.
    \end{enumerate}
\end{theorem}
\begin{proof}
  Let $x$ be a nonzero real number represented as
  \[x = m \cdot \be^e = (-1)^s\cdot(0.d_1d_2\dots d_nd_{n+1}\dots)\be^e\]
  where $d_1 \neq 0$. The smallest possible magnitude for the mantissa is $0.100\dots$ (in base $\be$). Therefore, the bounds on $m$ are
  \[\f 1\be \leq \abs{m} < 1.\]
  Since the floating point representation only stores $n$ digits, the last digit stored is $d_n$, which is in the $\be^{-n}$ position relative to the decimal point.

  Now, a chopped system truncates everything after $d_n$, and the absolute error would be given by
  \[\abs{x - \fl(x)} = 0.\underbrace{00 \dots 0}_{n \text{ zeros}} d_nd_{n+1}\dots \times \be^e < \be^{-n} \times \be^e = \be^{e-n}.\]
  Now consider the relative error.
  \[\abs{\ep} = \abs{\f{x - \fl(x)}{x}} < \f{\be^{e-n}}{\abs{m \times \be^e}} = \f{\be^{-n}}{\abs{m}}.\]
  To find the upper bound, we must minimize the denominator, whose minimum value we previously determined to be $1/\be$, which yields
  \[\abs{\ep} < \f{\be^{-n}}{1/\be} \imp \ep \leq \be^{-n+1}. \tag{a}\]

  In a rounded system, $\fl(x)$ is the number with $n$ digits closest to $x$. The quantity analogous to a ``least count'' would be $\be^{e-n}$. When rounding, the error cannot exceed half of this value
  \[\abs{x - \fl(x)} \leq \f 12 \be^{-n} \times \be^e = \f 12 \be^{e-n}.\]
  Dividing by $x$ yields
  \[\abs{\ep} = \f{\abs{x - \fl(x)}}{\abs{x}} \leq \f 12 \cdot \f{\be^{-n}}{\abs{m}}.\]
  Once more, the error is maximized at $\abs{m} = 1/\be$. Therefore,
  \[\ep \leq \f 12 \be^{-n+1}. \tag{b}\]
\end{proof}

\subsubsection*{Propogation of Errors}
When performing the arithmetic operations with approximate quantities, it is important to study the errors in the sum, difference, product, and quotient. Let $x = x^* + \ep$ and $y = y^* + \eta$, where $x$ and $y$ are true real values, and $x^*$ and $y^*$ are their approximations. Let $r_n$ denote the relative error in a quantity $n$. Then,
\begin{align*}
  r_{xy} &= \f{xy - x^*y^*}{xy} = \f{xy - (x - \ep)(y - \eta)}{xy} = \f \ep x + \f \eta y - \f{\ep\eta}{xy} \apx r_x + r_y, \\
  r_{x/y} &= \f{x(1 + r_x)}{y(1 + r_y)} = \f{1 + r_x}{1 + r_y} - 1 \approx (1 + r_x)(1 - r_y) - 1 \approx r_x - r_y.
\end{align*}
These approximations hold for $\abs{r_x}, \abs{r_y} \ll 1$. Errors in addition in subtraction are given by
\[r_{x \pm y} = r_x\left ( \f{x}{x \pm y}\right ) \pm r_y \left ( \f{y}{x \pm y} \right ).\]

In general, errors propogated through multiplication and division do not propogate rapidly. However, the propogated error in addition and subtraction can be much larger than the relative errors in either $x$ and $y$. This is known as a loss of significance or catastrophic cancellation.

\begin{example}
  Consider the function
  \[f : x \mapsto x(\sqrt{x + 1} - \sqrt{x}).\]
  Evaluate $f(10^5)$ using $6$-digit floating point arithmetic with rounding.
\end{example}
\begin{solution}
  Firstly, compute the floating point forms of the square roots:
  \begin{align*}
    \fl(\sqrt{100,001}) &= 3.16229 \times 10^2 \\
    \fl(\sqrt{100,000}) &= 3.16228 \times 10^2.
  \end{align*}
  Taking the difference would yield $0.00001 \times 10^2$. Starting wih $6$ significant digits, we are down to $1$, which is a huge loss in precision. The final result would be
  \[f(100,000) \apx (1.00000 \times 10^5) \times (1.00000 \times 10^{-3}) = 1.00000 \times 10^2 = 100.\]

  However, the true value of $f(10^5)$ is around 158.113488. To avoid the huge loss in precision, we first manipulate the function to remove the subtraction of two nearly equal quantities:
  \[f(x) = \f{x}{\sqrt{x + 1} + \sqrt{x}}\]
  Performing $6$-digit floating point arithmetic on the manipulated function gives $158.114$, which is much closer to the true value.
\end{solution}

We now consider the propogation of errors through function evaluation. Let $x = x^* + \ep$ where $x \in \bb R$ and $x^*$ is an approximation correct to $n$ significant figures. Let $f : X \to \bb R$ be a smooth function for some subset $X$ of $\bb R$. Using its Taylor series, we have
\[f(x^* + \ep) = f(x^*) + \ep f'(x^*) + \f{\ep^2}{2}f''(\al)\]
for some $\al \in [x, x^*]$. The relative error is given by
\[r_{f(x)} = \f{f(x) - f(x^*)}{f(x)} = \f{\ep f'(x^*) + \ep^2f''(\al)/2}{f(x)} \apx \ep \f{f'(x^*)}{f(x)}.\]
Assuming that $f(x)$ and $f(x^*)$ are relatively close,
\[r_{f(x)} \apx \f{\ep}{x} \cdot \f{xf'(x^*)}{f(x^*)} \imp \abs{r_{f(x)}}\apx \abs{r_x} \abs{\f{x^*f'(x^*)}{f(x^*)}}.\]

We also denote
\[\cl K = \abs{\f{x^*f'(x^*)}{f(x^*)}}\]
called the \emph{condition number}. If $\cl K$ is nearly $1$, then the error in $f(x)$ is called natural. If it is very large however, then the relative error in the evaluation of $f(x)$ will also be large.

Many numerical methods, especially in linear algebra, involve summations. Quantifying the error propagation in summations becomes an important topic of study. Consider the computation of the sum
\[S = \sum_{i=1}^m x_i,\]
where $x_i$ for $i \in 1 : m$ are floating point numbers. Define
\[S_2 = \fl(x_1 + x_2) = (x_1 + x_2)(1 + \ep_2)\]
and recusrively continue
\[S_{r+1} = \fl(S_r + x_{r+1}).\]
Expanding out the $n$th sum and neglecting $\ep_i\ep_j$ for $i, j \in 1 : n$, we get
\[S_n - (x_1 + x_2 + \cdots + x_n) \apx \sum_{i=1}^n \ep_i \sum_{j=1}^i x_i = \sum_{j=1}^n x_n\sum_{i=j}^n \ep_n.\]

\begin{example}
  Evaluate the polynomial
  \[f(x) = 1.107x^3 + 0.3129x^2 - 0.0172x + 1.1075\]
  for $x = 0.1234$ in nested form using $5$-digit floating point arithmetic with rounding.
\end{example}
\begin{solution}
  The polynomial is bracketed as
  \[f(x) = ((1.107x + 0.3129)x - 0.0172)x + 1.1075\]
  and evaluated from the inside-out according to the rules of 5-digit floating point arithmetic. This yields
  \[f(0.1234) \approx 1.1122.\]
\end{solution}

\section{Finding Roots for Nonlinear Equations}

Finding the roots of
\[f(x) = 0\]
for an arbitrary $f$ (polynomial or transcendental function) is critical to many scientific and engineering fields. This section talks about various methods to find roots.

\begin{definition}
  A function $f : R \to \bb R$ is said to be a real \emph{polynomial} if
  \[f(x) = a_nx^n + a_{n-1}x^{n-1} + \cdots + a_1x^1 + a_0\]
  for a fixed $n \in \bb N$, called its \emph{degree}.
\end{definition}

A real polynomial always has $n$ complex roots, up to repitition.

\begin{definition}
  A function $f : X \to \bb R$ is said to be \emph{transcendental} if its closed form contains trigonometric, exponential, or logarthmic functions.
\end{definition}

It is possible to construct algorithms that find roots of transcendental or polynomial equations, especially when it is not possible or impractical to find closed form roots. These algorithms are divided into two categories:
\begin{enumerate}
  \item simple enclosure methods, and
  \item fixed point iteration schemes.
\end{enumerate}
Simple closure methods are based on the intermediate value theorem (IVT) described below, and work by finding an interval that surely contains a root and shrinking it systematically.

\begin{theorem}[Intermediate Value Theorem]
  Let $f$ be a continuous function on $[a, b]$, and let $K \in \bb R$ be a real number in between $f(a)$ and $f(b)$. Then, there exists $c \in (a, b)$ such that
  \[f(c) = K.\]
\end{theorem}

\subsection{Bisection Method}
The simplest method is to cut the interval in half, determine which new subinterval contains a root utilizing the IVT, and repeat the process to a desired accuracy. This is fundamentally the \emph{bisection method}.

\begin{theorem}
  Let $f$ be a continuous function on $[a, b]$, and suppose $f(a)f(b) < 0$. The bisection method loosely defined above generates a sequence $\{p_n\} \to p \in (a, b)$ such that $f(p) = 0$ that satisfies
  \[\abs{p_n - p} \leq \f{b-a}{2^n}.\] 
\end{theorem}

Since $2^{-n} \to 0$, it follows that $\{p_n\} \to p$. We omit the formal proof here due to triviality, and focus on analyzing the \emph{rate of convergence}.

\begin{definition}
  Let $\{p_n\} \to p$. If there exists a sequence $\{\be\} \to 0$ and a positive constant $\la$ such that
  \[\abs{p_n - p} \leq \la \abs{\be_n},\]
  for sufficiently large values of $n$, then $\{p_n\}$ is said to converge to $p$ with \emph{rate of convergence} $O(\be_n)$.
\end{definition}

It is important for all algorithms to possess a stopping criterion, i.e. they need to end in a finite number of steps. The following implementation hard codes this cap if the root isn't within the tolerance range.

\begin{codebox}
\Procname{$\proc{Bisection Method }(f, a, b, \id{tol}, N)$}
\li \If $f(a) \cdot f(b) \geq 0$
\li     \Then \Error ``The root is not bracketed."
    \End
\li \For $j \gets 1$ \To $N$
\li     \Do $c \gets (a + b)/2$
\li         \If $|f(c)| < \id{tol}$
\li             \Then \Return $c$ \Comment{Convergence criterion met}
            \End
\li         \If $f(a) \cdot f(c) < 0$
\li             \Then $b \gets c$ \Comment{Root is in left half}
\li             \Else $a \gets c$ \Comment{Root is in right half}
            \End
    \End
\li \Return $c$ \Comment{Best approximation after $N$ iterations}
\end{codebox}
\emph{Bracketing} refers to bounding the root between a positive and negative number.
\end{document}
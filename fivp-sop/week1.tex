\documentclass[12pt]{article}
\usepackage[margin=1in, headheight=20pt]{geometry}
\usepackage{amsthm, amsmath, amssymb}
\usepackage{mathtools}
\usepackage[italicdiff]{physics}
\usepackage{enumitem}
\usepackage{lmodern}

\newcommand{\bb}[1]{\mathbb{#1}}
\newcommand{\cl}[1]{\mathcal{#1}}

\newcommand{\p}[1]{\left ( #1 \right )}
\newcommand{\bk}[1]{\left [ #1 \right ]}
\newcommand{\br}[1]{\left \{ #1 \}}
\newcommand{\ab}[1]{\langle #1 \rangle}

\newcommand{\f}[2]{\frac{#1}{#2}}
\newcommand{\nset}{\varnothing}
\newcommand{\oo}{\infty}
\DeclareMathOperator{\D}{D}
\DeclareMathOperator{\E}{E}
\DeclareMathOperator{\Var}{Var}

\newcommand{\al}{\alpha}
\newcommand{\gm}{\gamma}
\newcommand{\de}{\delta}
\newcommand{\De}{\Delta}
\newcommand{\ep}{\varepsilon}
\newcommand{\la}{\lambda}
\newcommand{\si}{\sigma}
\newcommand{\om}{\omega}
\newcommand{\Om}{\Omega}

\newcommand{\imp}{\Rightarrow}
\newcommand{\pmi}{\Leftarrow}
\renewcommand{\iff}{\Leftrightarrow}
\newcommand{\ffi}{\Rightarrow\!\Leftarrow}

\setlist[enumerate]{label=(\alph*)}

\newtheoremstyle{boldnote}
  {}
  {}
  {\itshape} 
  {}
  {\bfseries}
  {.}
  { }
  {\thmname{#1}\thmnumber{ #2}\thmnote{ (\bfseries #3)}}
\theoremstyle{boldnote}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}

\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}{Example}

\title{
    \textbf{Week 1: Stochastic Galerkin Method}
}
\author{
  Dhyan Laad \\
  \texttt{2024ADPS0875G}
}
\date{}

\begin{document}
\maketitle

\section*{Preliminaries}
Galerkin methods are a family of numerical techniques that are used to approximate solutions of continuous operator problems (such as differential equations, commonly partial differential equations) by converting them into algebraic systems that are easier to work with. The stochastic Galerkin method described here is used to remove uncertaintly introduced by a random variable in a fractional initial value problem of the form
\[\D^\al_{0, t} y(t, \xi) = a(\xi)y(t, \xi), \quad y(0, \xi) = y_0(\xi), \tag{1}\]
where $a : \Xi \to \bb R$ is a measurable function that depends on the parameter $\xi$ defined on some subset $\Xi$ of $\bb R$, which will be modelled as a random variable $\xi : \Om \to \Xi$ on some probability space $(\Om, \cl F, P)$.

In the case of the operator being the Caputo derivative, the solution to $(1)$ is given by
\[y(t, \xi) = y_0(\xi)E_\al(a(\xi)t^\al) \tag{2}\]
where $E_\al$ is the one-parameter family of Mittag-Leffler functions:
\[E_\al(z) = \sum_{k=0}^\oo \f{z^k}{\Gamma(\al k + 1)}.\]

Let the density of the probability measure $P$ be $\rho$. We are interested in expressing $(2)$ with a basis in the associated Hilbert space:
\[\cl L_2(\Xi, \rho) = \{f : \Xi \to \bb R : \text{$f$ is measurable and $\E(f^2) < \oo$}\}.\]
An orthnormal basis $\{\Phi_i\}$ of polynomials $\Phi_i : \Xi \to \bb R$ for $i \in \bb N_0$ can be constructed with the Gram-Schmidt process. Let $\Phi_0(x) = 1$, and $\deg(\Phi_i) = i$. It is possible to find a set of such orthogonal polynomials for a number of well-known probability distributions, although this is not always possible, and we continue under the assumption that the distribution may be modelled with such as a basis.

The representation of a function $f \in \cl L_2(\Xi, \rho)$ is called its general polynomial chaos (gPC) expansion:
\[f(\xi) = \sum_{i=0}^\oo f_i\Phi_i(\xi)\]
where $f_i = \ab{f, \Phi_i}$ are real coefficients. Applying this to $(2)$, its gPC expansion would be
\[y(t, \xi) = \sum_{i=0}^\oo \hat{v}_i(t)\Phi_i(\xi), \tag{3}\]
with coefficient functions $\hat{v}_i : [0, \oo) \to \bb R$. Truncating $(3)$ yields a finite approximation:
\[\hat{y}^{(n)}(t, \xi) = \sum_{i=0}^n \hat{v}_i(t)\Phi_i(\xi).\]
It holds that
\[\lim_{n \to \oo} \norm{y(t, \cdot) - \hat{y}^{(n)}(t, \cdot)} = 0\]
on the Hilbert space for all $t \geq 0$.

\section*{Stochastic Galerkin Method}
We now present the stochastic Galerkin method. The core idea is to approximate the unknown coefficient functions in the gPC expansion of $(2)$. Start by defining a truncated approximation:
\[\tilde{y}^{(n)}(t, \xi) = \sum_{i=0}^nv_i(t)\Phi_i(\xi) \tag{4}\]
where each $v_i$ approximates $\hat{v}_i$. Now inserting $(4)$ into $(1)$ would yield a residual function, which encapsulates the error. To minimize this error, we require the residual to be orthogonal to the subspace spanned by the basis $\{\Phi_0, \Phi_1, \dots , \Phi_n\}$ in the Hilbert space. Mathematically, the inner product of the residual and each basis polynomial must evaluate to $0$. This inner product has the effect of removing the stochastic component $\xi$ from the system, and results in a coupled linear system of deterministic FDEs:
\[\D^\al_{0,t} \vb v(t) = A\vb v(t) \tag{5}\]
where $\vb v(t)$ is the vector of unknown approximated coefficients: $(v_0(t), v_1(t), \dots , v_n(t))^\top$ and $A$ is a symmetric matrix where the $(i,j)$th entry $a_{ij}$ is $\ab{a(\xi)\Phi_i, \Phi_j}$. The system $(5)$ can now be solved numerically with appropriate initial values.

\subsection*{Convergence Analysis}
We impose three assumptions before analyzing the convergence of the approximations on an interval $[0, T]$ for $T > 0$.
\begin{enumerate}[label=A\arabic*.]
    \item For order $\al \in (0, 1)$, there is a fractional derivative with respect to $\D^\al_{0, t}$ for each term of the series $(3)$. Furthermore,
    \[\sum_{i=0}^\oo \hat{v}_i(t)\Phi_t(\xi) \quad \text{and} \quad \sum_{i=0}^\oo (\D^\al_{0,t} \hat{v}_i(t))\Phi_i(\xi)\]
    are uniformly convergent on $[0, T]$ for every $\xi \in \Xi$.
    \item The function $a$ as described in $(1)$ is essentially bounded.
    \item In the series $(3)$, the coefficient functions $\hat{v}_i(t)$ satisfy
    \[\lim_{n \to \oo} (n+1)\max_{t\in[0,T]} \sum_{i=n+1}^\oo \abs{\hat{v}_i(t)} = 0.\]
\end{enumerate}

The last axiom is to ensure the convergence of the series is at least greater than that of the harmonic series.

Now for some preliminary results.

\begin{lemma}
  If the function $a : \Xi \to \bb R$ is measurable and essentially bounded by a constant $C_a > 0$, then the entries of the matrix $A$ from $(5)$: $a_{ij}$ satisfy $\abs{a_{ij}} \leq C_a$ for all $i, j \in 1 : n$.
\end{lemma}

\begin{lemma}
  If the function $a : \Xi \to \bb R$ is measurable and essentially bounded by $a_{\mathit{min}} \leq a \leq a_{\mathit{max}}$ where $a_{\mathit{min}}$ and $a_{\mathit{max}}$ are constants, then each eigenvalue $\la$ of the matrix $A$ as described in $(5)$ satisfies $\la \in [a_{\mathit{min}}, a_{\mathit{max}}]$.
\end{lemma}

What follows is an outline of the logic of the proof to the central theorem.

\begin{theorem}
  Consider the linear FDE $(1)$ with the Caputo derivative operator. Let $y(t, \xi)$ be the solution to the equation whose exact gPC expansion is $(3)$, and $\tilde{y}^{(n)}(t, \xi)$ be the $(n+1)$th term gPC solution $(4)$ generated by the Galerkin system $(5)$. If the assumptions A1, A2, and A3 hold on an interval $[0, T]$ where $T > 0$, then
  \[\lim_{n \to \oo} \max_{t \in [0, T]} \norm{y(t, \cdot) - \tilde{y}^{(n)}(t, \cdot)}_{\cl L_2(\Xi, \rho)} = 0.\]
\end{theorem}

The error is firstly split in two:
 \[\norm{y(t, \cdot) - \tilde{y}^{(n)}(t, \cdot)}_{\cl L_2(\Xi, \rho)} \leq \norm{y(t, \cdot) - \hat{y}^{(n)}(t, \cdot)}_{\cl L_2(\Xi, \rho)} + \norm{\hat{y}^{(n)}(t, \cdot) - \tilde{y}^{(n)}(t, \cdot)}_{\cl L_2(\Xi, \rho)}.\]
 Since the first term on the right has been shown to converge to $0$ by construction, the problem is now reduced to showing that
 \[\lim_{n \to \oo} \norm{\hat{y}^{(n)}(t, \cdot) - \tilde{y}^{(n)}(t, \cdot)}_{\cl L_2(\Xi, \rho)} = 0\]
 for all $t \in [0, T]$.
  
 Let $\vb v^{(n)}(t)$ be the vector of unknown approximate coefficient functions as described in $(5)$, and $\vb{\hat{v}}^{(n)}(t)$ be the analogous vector of true coefficient functions. Then using Parseval's identity we have
 \[\norm{\hat{y}^{(n)}(t, \cdot) - \tilde{y}^{(n)}(t, \cdot)}^2_{\cl L(\Xi, \rho)} = \norm{\vb{\hat{v}}^{(n)} - \vb{v}^{(n)}(t)}_2^2 = \sum_{i=0}^n (\hat v_i(t) - v_i(t))^2.\]
 Define error functions $e_i(t) = \hat{v}_i(t) - v_i(t)$ for $i \in 0 : n$, and $\vb e^{(n)}(t) = (e_1(t), e_2(t), \dots , e_n(t))^\top$. Therefore, we must show that
 \[\lim_{n \to \oo}\norm{\vb e^{(n)}(t)}_2 = 0.\]
  
 From the original differential equation, the series representation of the solution,
 \[\D_{0,t}^\al y(t, \xi) = a(\xi)y(t, \xi) \imp \D_{0,t}^\al \p{\sum_{k=0}^\oo \hat{v}_k(t)\Phi_k(\xi)}=a(\xi)\p{\sum_{k=0}^\oo \hat{v}_k(t)\Phi_k(\xi)}.\]
 Utilizing A1, we may take the derivative operator inside the summation, and taking the inner product of both sides with an arbitrary basis polynomial $\Phi_i$, we have
 \[\D_{0,t}^\al \hat{v}_i = \sum_{k=0}^\oo a_{ki}\hat{v}_k(t). \tag{6}\]
  
 Taking the derivative of the error function $e_i$ and substituting $(5)$ and $(6)$ into it:
 \[\D_{0,t}^\al e_i(t) = \sum_{k=0}^n a_{ik}e_k(t) + \underbrace{\sum_{k=n+1}^\oo a_{ik}\hat{v}_k(t)}_{\hat{R_i}(t)}\]
 for $i \in 0 : n$. Let $\vb{\hat{R}}(t) = (\hat{R}_0, \hat{R}_1, \dots , \hat{R}_n)^\top$. Then the above equations may be represented as a system
 \[\D^\al_{0,t} \vb e(t) = A\vb e(t) + \vb{\hat{R}}(t).\]

 By the spectral decomposition theorem, there exists an orthogonal matric $P$ and a diagonal matrix $\Lambda$ of eigenvalues $\lambda_i$ for $i \in 0 : n$ such that $A = P\Lambda P^\top$. Define
 \[d_i(t) =\sum_{k=0}^n p_{ik}e_k(t)\]
 for every $i \in 0 : n$, where $p_{ik}$ is the $(i,k)$th term of the matrix $P$. This can also be represented as a system:
 \[\vb d(t) = P^\top \vb e(t). \tag{7}\]
 Applying the derivative operator on both sides gives us
 \[\D_{0,t}^\al \vb d(t) = \Lambda \vb d(t) + \vb R(t).\]
 where $\vb R(t) = (R_0(t), R_1(t), \dots , R_n(t))^\top$ is the residual vector for
 \[R_i(t) = \sum_{\ell=0}^n\sum_{k=n+1}^\oo p_{\ell i}a_{\ell k}\hat{v}_k(t).\]
 This decouples the equations
 \[\D^\al_{0, t} d_i(t) = \la_i d_i(t) + R_i(t)\]
 for $i \in 0 : n$. The initial values are $d_i(0) = 0$, and hence the solution to the decoupled equations are given by
 \[d_i(t) = \int_0^t (t - \tau)^{\al-1}E_{\al,\al}(\la_i(t - \tau)^\al)R_i(\tau)\dd{\tau}.\]
 Using A2 and Young's convolution inequality, it is possible to bound the functions
 \[\abs{d_i(t)} \leq C \f{T^{\al+1}}{\al}\max_{t \in [0, T]} \abs{R_i(t)}\]
 where
 \[C = \max \{\abs{E_{\al,\al}(-C_\al T^\al)}, \abs{E_{\al,\al}(C_aT^\al)}\}\]
 uniformly for all $t \in [0, T]$ and $i \in 0 : n$. It is also possible to now bound the residual vector in the infinity norm:
 \[\norm{\vb R(t)}_\oo \leq C_a\sqrt{n+1} \max_{t \in [0, T]} \sum_{k=n+1}^\oo \abs{\hat{v}_k(t)}\]
 uniformly for $t \in [0, T]$.

 From $(7)$, we have
 \[\norm{\vb e(t)}_2 \leq \norm{P}_2\norm{\vb d(t)}_2 \leq CC_a \f{T^{\al + 1}}{\al} (n+1)\max_{t \in [0, T]} \sum_{k=n+1}^\oo \abs{\hat{v}_k(t)}.\]
 From A3 we can now conclude that
 \[\lim_{n \to \oo} \max_{t\in[0,T]} \norm{\vb e(t)}_2 = 0.\]
\end{document}